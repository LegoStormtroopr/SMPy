\documentclass{article}
\usepackage{amsmath, amssymb, graphicx}
\begin{document}
\title{Survey Automation for the Crowd}
\author{Jessica Newman and Emma Tosch}
\maketitle
\section{Introduction}
Crowdsourcing platforms like Mechanical Turk have made it possible to harness the power of humans to answer difficult computer vision questions. They also offer a promising arena for social scientists to deliver surveys and questionnaires to a wide range of people. However, these platforms inherently introduce problems of quality control; the workers of Mechanical Turk are financially incentivized to respond quickly and lazily. We present a system for automating the design and presentation of surveys to ensure that the pool of responses is as representative as possible. A user of the system provides the questions for the survey, the options, and the question type. The system then presents the survey to a user base and attempts to remove causal bias introduced by question ordering, and eradicate unreliable respondents and outliers.
% explain the problem here
\section{Motivation}
In automating the delivery questions through a system like Mechanical Turk or SurveyMonkey, there are problems of quality control that must be addressed. To this end, Daniel Barowy at the University of Massachuetts created AutoMan \cite{automan}. AutoMan is a crowdprogramming system that allows programmers to use calls to Mechanical Turk's worker base as normal function calls. It also probabalistically eradicates unreliable or erroneous responses. (For example, if two workers are shown the same license plate, and submit the same license plate number as a response, the submitted number is correct with high probability) \cite{automan}. However, AutoMan does not yet provide support for surveys. AutoMan's quality control algorithm is heavily based on the idea that out of all the possible responses, there is a correct one. Therefore, the worker that picks an incorrect response is an adversary. This is necessarily untrue for surveys. Unlike the classification tasks of AutoMan, polls require that, at a minimum, a ranking be returned. In many cases, a distribution is required. Therefore, quality control must be handled in an entirely different manner. Additionally, surveys are order-sensitive. One question, such as "What is your opinion on gun control?", could have a significant impact on the respondent's answer to the following question, "Who do you plan to vote for in the next election?" These are challenges that need to be addressed in order to handle surveys correctly.
% explain shortcomings with current systems here

\section{Systems Contributions}
We provide a simulator for a survey respondent, called an 'agent.' An agent type, such as 'college student,' will respond to a particular question according to a certain distribution. Using this simulator we can demonstrate how our survey decides when to terminate taking samples; namely, when a convergence of distribution has been reached. %Still the same formula?

We also provide the capability to visualize response distributions to demonstrate the convergence of distribution.

\section{Survey Logic}
In our system, a survey is represented as a list of questions, each of which may have a list of options, (depending on the question type; free text responses will have no options.) When the surveys are presented, the questions will all be randomized. The survey creator may also elect to randomize the options. In this way we reduce causal bias.
	
% explain how surveys are generated here

\subsection{Quality Control}
% explain QC mechanism here
Our quality control mechanism is inspired by Efron's bootstrap methods for identifying outliers \cite{bootstrap}.

At the moment, we discard lazy respondents by immediately discarding respondents who pick the same answer for each question. In the future we hope to discard responses whose response contains a sequence of answers that lie outside a 95\% confidence interval. %more detail? Entropy?
\section{Related Work}
\subsection{Survey Software}
% Automan, whatever features Survey Monkey requires here
Social scientists have been using Mechanical Turk to post surveys, but lack an automated means of ensuring quality control and statistical power, so they have come up with their own methods of improving the quality of responses \cite{mturk_guide}.

AutoMan does not support surveys, and at its core it is a system designed for use by programmers, not social scientists, (with human computation represented as a normal function call) \cite{automan}. Incorporating surveys could be of some use, as well as making the interface more accessible to non-programmers.

SurveyMonkey provides free randomization of questions and options at this time, but in the best practices section of their website they recommend an explicit formula based on population size to know how many responses to take rather than aiming for a convergence of distribution \cite{surveymonkey_how_many}.
\subsection{Survey Design}
A 2009 paper was published on detecting waning attention of survey participants, which can very easily be extrapolated to checking for bots or lazy, rushed workers in Mechanical Turk \cite{satisficing}.
% Mathy things here
\section{Future Work}
\begin{itemize}
\item Other Media:  Many users of surveys require feedback about media other than text. For example, phonology researchers need to post audio files for participants to listen. We would eventually like a system that can post survey questions other than text.
\item Scrollbar: This would allow for continuous distributions. Also, it provides another mechanism for adversary identification. A lazy adversary would not alter the position of the scroll bar.
\item Redundancy: One of the most useful methods for adversary identification is the introduction of redundant questions. A reliable respondant is expected to answer redundant questions in a more-or-less consistent manner \cite{satisficing}. (For instance- the questions 'what year were you born?' and 'how old are you?') Any respondent who does not answer these questions consistently can reasonably be labeled an adversary. Eventually our system will provide some way for the survey maker to label questions as redundant and mark which options are consistent.
\end{itemize}
\bibliography{ourbib}
\bibliographystyle{plain}
\end{document}
